{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment & config loaded\n"
     ]
    }
   ],
   "source": [
    "# *********\n",
    "# Environment & Settings\n",
    "# *********\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"‚ùå OPENAI_API_KEY missing\")\n",
    "\n",
    "# --------- GLOBAL CONFIG ----------\n",
    "EMBEDDING_DIM = 1536\n",
    "SIMILARITY_THRESHOLD = 0.30\n",
    "MAX_CACHE_SIZE = 100   # LRU limit\n",
    "DB_PATH = \"agent_memory.db\"\n",
    "\n",
    "print(\"‚úÖ Environment & config loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e184e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heman\\Desktop\\Agentic AI Projects\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# ********\n",
    "# LLM Initialization\n",
    "# ********\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff790e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings ready\n"
     ]
    }
   ],
   "source": [
    "# *********\n",
    "# Embeddings\n",
    "# *********\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embeddings ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc1c1f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Per-session fact memory ready\n"
     ]
    }
   ],
   "source": [
    "# ********\n",
    "# SQLite (FACT MEMORY + SESSION STORAGE)\n",
    "# ********\n",
    "\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS facts (\n",
    "    session_id TEXT PRIMARY KEY,\n",
    "    data TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "def load_facts(session_id: str) -> dict:\n",
    "    cursor.execute(\"SELECT data FROM facts WHERE session_id=?\", (session_id,))\n",
    "    row = cursor.fetchone()\n",
    "    return json.loads(row[0]) if row else {}\n",
    "\n",
    "def save_facts(session_id: str, facts: dict):\n",
    "    cursor.execute(\n",
    "        \"REPLACE INTO facts (session_id, data) VALUES (?, ?)\",\n",
    "        (session_id, json.dumps(facts))\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "print(\"‚úÖ Per-session fact memory ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce107a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******\n",
    "# FAISS + LRU Cache\n",
    "# *******\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "\n",
    "# LRU cache: {vector_index: AIMessage}\n",
    "CACHE = OrderedDict()\n",
    "\n",
    "def embed(text: str) -> np.ndarray:\n",
    "    return np.array(embeddings.embed_query(text), dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04912efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********\n",
    "# Fact Extraction Prompt\n",
    "# *********\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "FACT_EXTRACT_PROMPT = SystemMessage(\n",
    "    content=\"\"\"\n",
    "Extract factual information from user input.\n",
    "\n",
    "Return ONLY valid JSON.\n",
    "If none, return {}.\n",
    "\n",
    "Fields:\n",
    "- name\n",
    "- likes\n",
    "- dislikes\n",
    "- profession\n",
    "- location\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078bcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********\n",
    "# Fact Extraction Function\n",
    "# ********\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def extract_facts(user_text: str, session_id: str):\n",
    "    facts = load_facts(session_id)\n",
    "\n",
    "    response = llm.invoke([\n",
    "        FACT_EXTRACT_PROMPT,\n",
    "        HumanMessage(content=user_text)\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        new_facts = json.loads(response.content)\n",
    "        if isinstance(new_facts, dict):\n",
    "            facts.update(new_facts)\n",
    "            save_facts(session_id, facts)\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5802669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********\n",
    "# Fact QA (STRICT)\n",
    "# ********\n",
    "\n",
    "FACT_QA_PROMPT = SystemMessage(\n",
    "    content=\"\"\"\n",
    "Answer ONLY from provided facts.\n",
    "If missing, respond: NOT_FOUND\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def answer_from_facts(user_text: str, session_id: str):\n",
    "    facts = load_facts(session_id)\n",
    "    if not facts:\n",
    "        return None\n",
    "\n",
    "    response = llm.invoke([\n",
    "        FACT_QA_PROMPT,\n",
    "        HumanMessage(content=f\"Facts: {facts}\\nQuestion: {user_text}\")\n",
    "    ])\n",
    "\n",
    "    if response.content.strip() == \"NOT_FOUND\":\n",
    "        return None\n",
    "\n",
    "    return AIMessage(content=response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f859a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********\n",
    "# Confidence Check (ANTI-HALLUCINATION)\n",
    "# **********\n",
    "\n",
    "CONFIDENCE_PROMPT = SystemMessage(\n",
    "    content=\"\"\"\n",
    "Is the following answer factual and confident?\n",
    "Reply ONLY YES or NO.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def is_confident(answer: str) -> bool:\n",
    "    response = llm.invoke([\n",
    "        CONFIDENCE_PROMPT,\n",
    "        HumanMessage(content=answer)\n",
    "    ])\n",
    "    return response.content.strip().upper() == \"YES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a730dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****\n",
    "# Agent State\n",
    "# *****\n",
    "\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: Annotated[List, add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258ba459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********\n",
    "# MAIN AGENT LOGIC (Production Core)\n",
    "# **********\n",
    "\n",
    "import time\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def chatbot(state: AgentState, session_id: str) -> AgentState:\n",
    "    last_msg = state[\"query\"][-1]\n",
    "\n",
    "    if not isinstance(last_msg, HumanMessage):\n",
    "        return state\n",
    "\n",
    "    user_text = last_msg.content\n",
    "    start = time.time()\n",
    "\n",
    "    # 1Ô∏è‚É£ FACT MEMORY\n",
    "    fact_answer = answer_from_facts(user_text, session_id)\n",
    "    if fact_answer:\n",
    "        print(f\"‚ö° FACT HIT ({(time.time()-start)*1000:.1f} ms)\")\n",
    "        state[\"query\"].append(fact_answer)\n",
    "        return state\n",
    "\n",
    "    # 2Ô∏è‚É£ SEMANTIC CACHE\n",
    "    vec = embed(user_text).reshape(1, -1)\n",
    "    if index.ntotal > 0:\n",
    "        dist, idx = index.search(vec, 1)\n",
    "        if dist[0][0] < SIMILARITY_THRESHOLD:\n",
    "            key = idx[0][0]\n",
    "            if key in CACHE:\n",
    "                CACHE.move_to_end(key)\n",
    "                print(\"‚ö° CACHE HIT\")\n",
    "                state[\"query\"].append(CACHE[key])\n",
    "                return state\n",
    "\n",
    "    # 3Ô∏è‚É£ LLM\n",
    "    print(\"üê¢ LLM reasoning...\")\n",
    "    response = llm.invoke(state[\"query\"])\n",
    "\n",
    "    # 4Ô∏è‚É£ STORE (SAFE)\n",
    "    extract_facts(user_text, session_id)\n",
    "\n",
    "    if is_confident(response.content):\n",
    "        index.add(vec)\n",
    "        CACHE[index.ntotal - 1] = response\n",
    "        if len(CACHE) > MAX_CACHE_SIZE:\n",
    "            CACHE.popitem(last=False)\n",
    "\n",
    "    state[\"query\"].append(response)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb75af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******\n",
    "# Session ID\n",
    "# *******\n",
    "\n",
    "import uuid\n",
    "\n",
    "SESSION_ID = str(uuid.uuid4())\n",
    "# print(\"üÜï Session:\", SESSION_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9884cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Production graph built\n"
     ]
    }
   ],
   "source": [
    "# ******\n",
    "# LangGraph Build\n",
    "# ******\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"chatbot\", lambda s: chatbot(s, SESSION_ID))\n",
    "graph.add_edge(START, \"chatbot\")\n",
    "graph.add_edge(\"chatbot\", END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n",
    "print(\"‚úÖ Production graph built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******\n",
    "# Chat Interface\n",
    "# *******\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def chat(text: str):\n",
    "    result = workflow.invoke(\n",
    "        {\"query\": [HumanMessage(content=text)]}\n",
    "    )\n",
    "\n",
    "    for msg in result[\"query\"][-2:]:\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "        print(f\"{role}: {msg.content}\")\n",
    "\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59961674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê¢ LLM reasoning...\n",
      "User: My name is Hemant and I like cricket\n",
      "AI: That's great, Hemant! Cricket is a popular sport enjoyed by many people around the world. Do you have a favorite team or player?\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat(\"My name is Hemant and I like cricket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82a8dcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° FACT HIT (712.4 ms)\n",
      "User: What is my name?\n",
      "AI: Your name is Hemant.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d4ab5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° FACT HIT (912.2 ms)\n",
      "User: What do I like?\n",
      "AI: You like cricket.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat(\"What do I like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad490ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê¢ LLM reasoning...\n",
      "User: What is LangGraph?\n",
      "AI: LangGraph is a graph-based programming language that allows users to create and manipulate graphs using a simple and intuitive syntax. It is designed to make working with graphs easier and more efficient, particularly for tasks such as data analysis, network modeling, and algorithm development. LangGraph provides a range of built-in functions and libraries for working with graphs, as well as the ability to define custom functions and algorithms.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d1f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
